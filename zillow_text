import requests
import re
import json
import pandas as pd

import warnings
warnings.filterwarnings('ignore')
req_headers = {
    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
    'accept-encoding': 'gzip, deflate, br',
    'accept-language': 'en-US,en;q=0.8',
    'upgrade-insecure-requests': '1',
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'
}

city = 'saltlakecity-ut/' #*****change this city to what you want!!!!*****
data_list = []
urls = []
for page in range(1,20):
    if page == 1:
        url = 'https://www.zillow.com/homes/for_sale/'+city
    else:
        url = 'https://www.zillow.com/homes/for_sale/'+city+str(page)+'_p/'
    with requests.Session() as s:
        r = s.get(url, headers=req_headers)
        data = json.loads(re.search(r'!--(\{"queryState".*?)-->', r.text).group(1))
    urls.append(url)
    data_list.append(data)

df = pd.DataFrame()

def make_frame(frame):
    for i in data_list:
        for item in i['cat1']['searchResults']['listResults']:
            frame = frame.append(item, ignore_index=True)
    return frame

df = make_frame(df)

#filter unneccesary columns
df = df.drop('hdpData', 1) #remove this line to see a whole bunch of other random cols, in dict format

#get rid of duplicates
df = df.drop_duplicates(subset='zpid', keep="last")
df['area'] = df['area'].fillna(1)
#filters
df['zestimate'] = df['zestimate'].fillna(0)
df['best_deal'] = df['unformattedPrice'] - df['zestimate']
df['price per sq ft'] = df['unformattedPrice'] / df['area']
df = df.sort_values(by='best_deal',ascending=True)

print('shape:', df.shape)
homes = df[['address','beds','baths','area','price','zestimate','best_deal', 'price per sq ft', 'detailUrl']]

## READ TO CSV FILE

compression_opts = dict(method='zip',
                        archive_name='slchomes_10_19.csv')  
homes.to_csv('slchomes_10_19.zip', index=False,
          compression=compression_opts) 